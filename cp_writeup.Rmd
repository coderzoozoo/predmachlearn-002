# Predicting Qualitative Activity Recognition of Weight Lifting Exercise

## Synopsis
This document describes a model to predict the manner in which Dumbbell Biceps Curl is done. The goal of this analysis is to predict how well the exercise is done. The analysis uses the data (Source: http://groupware.les.inf.puc-rio.br/har) from accelerometers on the belt, forearm, arm, and dumbbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information about the data is available at http://groupware.les.inf.puc-rio.br/har.

The analysis cleans the data, creates tidy set, does Princial Component Analysis on the data. The data is trained using Random Forest model  and cross validated using K-Fold cross validation. Model, generated through training, is then used to predict the estimate of accuracy by predicting on a subset of training data, sliced from training data before training the model. Many training algorithms, such as boosting with trees, linear models, random forest with random sampling etc, were tried during the analysis. Random Forest with K-Fold cross validation worked out to be the best. The model is then used to predict 20 observation. 19 of the 20 samples were correctly predicted.

## Data Processing

Download data

```{r setoptions, echo=FALSE}
library(knitr)
opts_chunk$set(echo = TRUE)
opts_chunk$set(cache = TRUE)
```

```{r downloadcsv}
setwd("~/Documents/DataScience/code/predmachlearn-002/")
trainfileurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testfileurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(trainfileurl, "pml-training.csv", method="curl")
download.file(testfileurl, "pml-testing.csv", method="curl")
```

Read data
```{r exploretraindata}
trainDF <- read.csv("pml-training.csv")
testDF <- read.csv("pml-testing.csv")
```

Prediction model should be independent of all the predictors that are very specific to the experiment. Our model should be able to predict the outcome if same experiment is done again with different participants. Only the predictors that come from accelerators should be used. This hypothesis would result in a more generalized model.
Let us do pair plot for the predictors that are specific to this experiment and outcome. 
```{r pairplot}
pairs(classe~X+user_name+cvtd_timestamp, data=trainDF)
```

Since we see no correlation between the predictors and outcome, we can remove these predictors from the training set.
```{r subsetdf}
outcomeTrainVect <- trainDF$classe
trainDF <- subset(trainDF, select = -c(classe, X, user_name, cvtd_timestamp))
testDF <- subset(testDF, select = -c(problem_id,  X, user_name, cvtd_timestamp))
```

Remove zero variance predictors from the test and training data frame.

```{r removenearzerocols}
require(caret)
nsv <- nearZeroVar(trainDF, saveMetrics = TRUE)
trainDF <- subset(trainDF, select=colnames(trainDF)[!nsv[["nzv"]]])
testDF <- subset(testDF, select=colnames(testDF)[!nsv[["nzv"]]])
```

Let us check the number of training examples that do not have missing values.
```{r missingvalues}
nrow(trainDF[complete.cases(trainDF), ])
```

We can see that there are only 406 observation sets that do not have missing values. It would be difficult to train a model using 406 observation sets because lesser training examples would result in high bias. So, lets  impute missing values using 'knnImpute'. 
```{r imputemissingvalues}
require(RANN)
preObj1 <- preProcess(trainDF, method = "knnImpute")
trainDfPred1 <- predict(preObj1, trainDF)
testDfPred1 <- predict(preObj1, testDF)
trainDF1 <- data.frame(trainDfPred1)
testDF1 <- data.frame(testDfPred1)
```

```{r numpredictors}
length(colnames(trainDF))
length(colnames(testDF))
```

There are still high number of predictors. We can create better predictors, which would explain more variance,  using Principal Component Analysis (PCA). PCA would also compress the data.

```{r dopca}
preObj2 <- preProcess(trainDF1, method = "pca")
trainPC <- predict(preObj2, trainDF1)
testPC <- predict(preObj2, testDF1)
trainDF2 <- data.frame(trainPC)
testDF2 <- data.frame(testPC)
str(trainDF2)
```

We can see that the number of predictors are reduced from 96 to 32.

Lets us train our model using Random Forest algorithm because it predicts the outcome with high accuracy. Since, the size of the training set is relatively smaller, Random Forest algorithm's drawback of lesser execution speed can be ignored. We should expect higher accuracy percentage than 95%.

K-Fold cross validation is used during the training. Default value of 10 folds is used in the analysis. As quoted by Wikipedia, "The advantage of this method (K-Fold Cross Validation) over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once."
```{r train}
trainDF2 <- data.frame(classe=outcomeTrainVect, trainDF2)
inTrain <- createDataPartition(y=trainDF2$classe, p=0.7, list=FALSE)
training <- trainDF2[inTrain, ]
testing <- trainDF2[-inTrain, ]
library(randomForest)
set.seed(2009)
model <- train(classe~., data=training, trControl = trainControl(method = "cv") ,method="rf")
```

Predict the 'testing' set and check the confusion matrix
```{r predict}
pred <- predict(model, testing)
confusionMatrix(pred, testing$classe)
```
The estimated accuracy is 96.65%.

Plot the model accuracy, Cohen's Kappa, and resampleHist to understand accuracy of our model
```{r plotmodel}
par(oma = c(0,0,2,0))
plot(model,  main = "Figure 1")
plot(model, metric="Kappa", main = "Figure 2")
resampleHist(model, type="hist", main = "Figure 3")
```

We can see in Figure 1 and Figure 2 that both accuracy and Kappa increase as the number of predictors increased. Figure 3 explains that almost all the samples could be predicted with more than 95% accuracy. Kappa histogram depicts almost perfect inter observer agreement. 

## Result
Predict the outcome in testDF2
```{r predTestset}
predTest <- predict(model, testDF2)
print(predTest)
```